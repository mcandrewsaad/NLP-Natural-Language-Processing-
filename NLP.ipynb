{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mcand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing the libraries\n",
    "import bz2\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the bz2 file and transforming it to a dataframe\n",
    "\n",
    "train_file = bz2.BZ2File(\"train.ft.txt.bz2\")    \n",
    "line_list = train_file.readlines()    \n",
    "lines = [x.decode('utf-8') for x in line_list]    \n",
    "\n",
    "# Split in two: sentiment and review    \n",
    "sentiment = [review.split(\"__label__\")[1][0] for review in lines]\n",
    "reviews = [review.split(\"__label__\")[1][1:]  for review in lines]\n",
    "   \n",
    "newlist = []\n",
    "    \n",
    "for i in range(len(sentiment)):\n",
    "    \n",
    "    newlist.append([sentiment[i], reviews[i]])\n",
    "\n",
    "df = pd.DataFrame(newlist, columns = ['score', 'review'] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuation\n",
    "The preprocessed reviews are further cleaned by dropping punctuations. Using regular expressions, only whitespaces and alphanumeric characters are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing punctation\n",
    "pattern = r\"[^\\w\\s]\"\n",
    "df[\"review\"] = df[\"review\"].str.replace(pat=pattern, repl=\" \", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to lowercase\n",
    "Every letter is also converted to lower case. This makes it so that \"iPhone\" will not be distinguishable from \"iphone\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to lowercase\n",
    "df[\"review\"] = df[\"review\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words\n",
    "\n",
    "\n",
    "Stop words consist of the most commonly used words that include pronouns (e.g. us, she, their), articles (e.g. the), and prepositions (e.g. under, from, off). These words are not helpful in distinguishing a document from another and are therefore dropped.\n",
    "\n",
    "Note that the stop_words were stripped of punctuations just as what we have done to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "stop_words = [word.replace(\"\\'\", \"\") for word in stop_words]\n",
    "\n",
    "remove_stop_words = lambda row: \" \".join([token for token in row.split(\" \") \\\n",
    "                                          if token not in stop_words])\n",
    "    \n",
    "df[\"review\"] = df[\"review\"].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing extra spaces\n",
    "\n",
    "Again, we make use of regular expressions to ensure we never get more than a single whitespace to separate words in our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing extra spaces\n",
    "pattern = r\"[\\s]+\"\n",
    "df[\"review\"] = df[\"review\"].str.replace(pat=pattern, repl=\" \", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of tokens in each review\n",
    "\n",
    "adding extra features to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of tokens\n",
    "df['n_tokens'] = df['review'].str.lower().str.split().apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling + detecting the language of each review\n",
    "\n",
    "the dataset is actually huge and taking a lot of computational time to execute the language detection call. I decided to take a random sample of the dataset with only around 50000 records just for the sake of the assignment and time consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling\n",
    "df2 = df.sample(n=50000)\n",
    "#detect language\n",
    "df2['language'] = df2['review'].apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>review</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149741</th>\n",
       "      <td>1</td>\n",
       "      <td>battery drains really fast charged thing day ...</td>\n",
       "      <td>14</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531604</th>\n",
       "      <td>1</td>\n",
       "      <td>poor blu ray transfer fans film warned absolu...</td>\n",
       "      <td>63</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694190</th>\n",
       "      <td>2</td>\n",
       "      <td>energetic fun classical music expert stretch ...</td>\n",
       "      <td>56</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910848</th>\n",
       "      <td>2</td>\n",
       "      <td>lovely tablecloth 70 x 86 oval tablecloth siz...</td>\n",
       "      <td>30</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3446703</th>\n",
       "      <td>1</td>\n",
       "      <td>company unresponsive placed order oct 4th 201...</td>\n",
       "      <td>31</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        score                                             review  n_tokens  \\\n",
       "149741      1   battery drains really fast charged thing day ...        14   \n",
       "531604      1   poor blu ray transfer fans film warned absolu...        63   \n",
       "1694190     2   energetic fun classical music expert stretch ...        56   \n",
       "910848      2   lovely tablecloth 70 x 86 oval tablecloth siz...        30   \n",
       "3446703     1   company unresponsive placed order oct 4th 201...        31   \n",
       "\n",
       "        language  \n",
       "149741        en  \n",
       "531604        en  \n",
       "1694190       en  \n",
       "910848        en  \n",
       "3446703       en  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2[['review','n_tokens','language']]\n",
    "y = df2[['score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization using CountVectorizer\n",
    "\n",
    "The classical approach in expressing text as a set of features is getting the token frequency. Each entry to the dataframe is a document while each column corresponds to every unique token in the entire corpora. The row will identify how many times a word appears in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1000, min_df=5, max_df=0.7)\n",
    "train_vector = vectorizer.fit_transform(X_train[\"review\"])\n",
    "test_vector = vectorizer.transform(X_test[\"review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreating the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.DataFrame(train_vector.A, columns=vectorizer.get_feature_names())\n",
    "train_df['n_tokens'] = X_train['n_tokens'].values\n",
    "train_df['language'] = X_train['language'].values\n",
    "train_df['language_flag'] = train_df.language.apply(lambda language: 1 if 'en' in language else 0)\n",
    "\n",
    "train_df = train_df.drop(['language'], axis=1)\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame(test_vector.A, columns=vectorizer.get_feature_names())\n",
    "test_df['n_tokens'] = X_test['n_tokens'].values\n",
    "test_df['language'] = X_test['language'].values\n",
    "test_df['language_flag'] = test_df.language.apply(lambda language: 1 if 'en' in language else 0)\n",
    "\n",
    "test_df = test_df.drop(['language'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcand\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\mcand\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clr = LogisticRegression()\n",
    "clr.fit(train_df, y_train)\n",
    "\n",
    "y_pred = clr.predict(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model reporting and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4241  817]\n",
      " [ 660 4282]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "#confusion matrix\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.87      0.84      0.85      5058\n",
      "           2       0.84      0.87      0.85      4942\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8523\n"
     ]
    }
   ],
   "source": [
    "#accuracy score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
